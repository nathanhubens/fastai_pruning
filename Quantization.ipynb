{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2066,  0.3966,  0.5587,  1.1892],\n",
       "        [ 0.3932,  1.6257, -0.6623, -0.0903],\n",
       "        [-0.8323, -0.0078,  1.2867, -0.1683],\n",
       "        [ 3.2285,  1.6256,  0.6275, -1.0032]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_f = torch.randn((4, 4)); x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x, n_bits):\n",
    "    \n",
    "    n_levels = 2**n_bits\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    q_min, q_max = 0, n_levels-1\n",
    "    \n",
    "    scale = (x_max - x_min) / (q_max - q_min)\n",
    "    print(scale)\n",
    "    zp = q_min - x_min / scale\n",
    "    \n",
    "    x_int = torch.round(x/scale) + zp\n",
    "    return torch.clamp(x_int, q_min, q_max), scale, zp\n",
    "    \n",
    "\n",
    "def dequantize(x, scale, zp):\n",
    "    return (x - zp)*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2066,  0.3966,  0.5587,  1.1892],\n",
       "        [ 0.3932,  1.6257, -0.6623, -0.0903],\n",
       "        [-0.8323, -0.0078,  1.2867, -0.1683],\n",
       "        [ 3.2285,  1.6256,  0.6275, -1.0032]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0213)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000, 122.5259, 129.5259, 159.5259],\n",
       "        [121.5259, 179.5259,  72.5259,  99.5259],\n",
       "        [ 64.5259, 103.5259, 163.5259,  95.5259],\n",
       "        [254.5259, 179.5259, 132.5259,  56.5259]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q, s, zp = quantize(x_f, 8); x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2066,  0.4050,  0.5542,  1.1936],\n",
       "        [ 0.3837,  1.6199, -0.6607, -0.0853],\n",
       "        [-0.8312,  0.0000,  1.2788, -0.1705],\n",
       "        [ 3.2184,  1.6199,  0.6181, -1.0018]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequantize(x_q, s, zp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer():\n",
    "    \n",
    "    def __init__(self, n_bits):\n",
    "        self.n_bits = n_bits\n",
    "    \n",
    "    def quantize_model(self, model):\n",
    "        pass\n",
    "    \n",
    "    def quantize_layer(self, layer):\n",
    "        W = layer.weights.data\n",
    "        b = layer.bias.data\n",
    "        \n",
    "    def _quantize(self, x, mode='asymmetric'):\n",
    "        \n",
    "        n_levels = 2**self.n_bits\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        \n",
    "        if mode == 'asymmetric':\n",
    "            q_min, q_max = 0, n_levels-1\n",
    "            self.scale = (x_max - x_min) / (q_max - q_min)\n",
    "            self.zp = int(q_min - x_min / self.scale)\n",
    "\n",
    "        elif mode == 'symmetric':\n",
    "            q_min, q_max = -n_levels/2, (n_levels/2)-1\n",
    "            self.scale = (x_max - x_min) / (q_max - q_min)\n",
    "            self.zp = 0            \n",
    "            \n",
    "        else: \n",
    "            raise NotImplementedError\n",
    "    \n",
    "        x_int = torch.round(x/self.scale) + self.zp\n",
    "        \n",
    "        return torch.clamp(x_int, q_min, q_max)\n",
    "    \n",
    "    def _dequantize(self, x):\n",
    "        return (x - self.zp)*self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Quantizer(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2066,  0.3966,  0.5587,  1.1892],\n",
       "        [ 0.3932,  1.6257, -0.6623, -0.0903],\n",
       "        [-0.8323, -0.0078,  1.2867, -0.1683],\n",
       "        [ 3.2285,  1.6256,  0.6275, -1.0032]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-104.,   19.,   26.,   56.],\n",
       "        [  18.,   76.,  -31.,   -4.],\n",
       "        [ -39.,    0.,   60.,   -8.],\n",
       "        [ 127.,   76.,   29.,  -47.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q = q._quantize(x_f, 'symmetric'); x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2167,  0.4050,  0.5542,  1.1936],\n",
       "        [ 0.3837,  1.6199, -0.6607, -0.0853],\n",
       "        [-0.8312,  0.0000,  1.2788, -0.1705],\n",
       "        [ 2.7069,  1.6199,  0.6181, -1.0018]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q._dequantize(x_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, mnist=True):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0280,  0.0547,  0.0385,  0.1155, -0.0160, -0.0367,  0.0064,  0.0154,\n",
       "         0.0815, -0.0021,  0.0553, -0.0576, -0.0516, -0.0505, -0.1036, -0.0582,\n",
       "        -0.0221,  0.0859, -0.0632,  0.0949])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer():\n",
    "    \n",
    "    def __init__(self, n_bits):\n",
    "        self.n_bits = n_bits\n",
    "    \n",
    "    def quantize_model(self, model):\n",
    "        for k, m in enumerate(model.modules()):\n",
    "            if not isinstance(m, Net):\n",
    "                W_q, b_q = self.quantize_layer(m)\n",
    "                m.weight.data  = W_q\n",
    "                m.bias.data = b_q\n",
    "            \n",
    "        return model    \n",
    "    \n",
    "    def quantize_layer(self, layer, mode='asymmetric'):\n",
    "        W = layer.weight.data\n",
    "        b = layer.bias.data\n",
    "\n",
    "        W_q = self._quantize(W, mode=mode)\n",
    "        b_q = self._quantize(b, mode=mode)\n",
    "        \n",
    "        return W_q, b_q\n",
    "    \n",
    "    def _compute_params(self, x_min, x_max, q_min, q_max):\n",
    "        \n",
    "        scale = (x_max - x_min) / (q_max - q_min)\n",
    "        initial_zp = q_min - x_min / scale\n",
    "\n",
    "        zp = 0\n",
    "        if initial_zp < q_min:\n",
    "            zp = q_min\n",
    "        elif initial_zp > q_max:\n",
    "            zp = q_max\n",
    "        else:\n",
    "            zp = initial_zp\n",
    "\n",
    "        return scale, int(zp)\n",
    "        \n",
    "        \n",
    "    def _quantize(self, x, mode='asymmetric'):\n",
    "        \n",
    "        n_levels = 2**self.n_bits\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        \n",
    "        if mode == 'asymmetric':\n",
    "            q_min, q_max = 0, n_levels-1\n",
    "            self.scale, self.zp = self._compute_params(x_min, x_max, q_min, q_max)\n",
    "\n",
    "        elif mode == 'symmetric':\n",
    "            q_min, q_max = -n_levels/2, (n_levels/2)-1\n",
    "            self.scale, self.zp = self._compute_params(x_min, x_max, q_min, q_max)\n",
    "            self.zp = 0\n",
    "        else: \n",
    "            raise NotImplementedError\n",
    "    \n",
    "        x_q = torch.round(x/self.scale) + self.zp\n",
    "        \n",
    "        return torch.clamp(x_q, q_min, q_max)\n",
    "    \n",
    "    def _dequantize(self, x):\n",
    "        return (x - self.zp)*self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Quantizer(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-104.,   19.,   26.,   56.],\n",
       "        [  18.,   76.,  -31.,   -4.],\n",
       "        [ -39.,    0.,   60.,   -8.],\n",
       "        [ 127.,   76.,   29.,  -47.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q._quantize(x_f, 'symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[201.,  37., 117.,  45., 194.],\n",
       "           [ 88., 172., 251.,  73., 170.],\n",
       "           [  0.,  37., 218.,  36., 214.],\n",
       "           [206., 158.,  17., 173.,  37.],\n",
       "           [100.,  71.,  60., 109.,  88.]],\n",
       " \n",
       "          [[102.,  83.,  73., 202.,  69.],\n",
       "           [181.,  29., 107.,  10.,  18.],\n",
       "           [ 19.,  61., 168., 105., 139.],\n",
       "           [ 11., 237., 116., 147., 156.],\n",
       "           [ 96., 159.,  65., 155., 229.]],\n",
       " \n",
       "          [[ 48.,  27., 152., 154., 130.],\n",
       "           [194., 136.,  84.,  12.,  57.],\n",
       "           [152., 203.,   3., 225., 122.],\n",
       "           [ 80.,  82., 198., 247., 225.],\n",
       "           [137.,   9., 224.,  81.,  41.]]],\n",
       " \n",
       " \n",
       "         [[[ 36., 197., 181., 186., 163.],\n",
       "           [ 16., 124.,  76., 242.,  85.],\n",
       "           [ 47., 228., 125., 136., 171.],\n",
       "           [236.,  38.,  65., 181., 115.],\n",
       "           [187.,  42., 133.,  10.,   2.]],\n",
       " \n",
       "          [[121., 238., 146., 108.,  98.],\n",
       "           [186., 188., 237.,  79.,  83.],\n",
       "           [204.,  66., 130., 219., 127.],\n",
       "           [158.,  43., 233., 198., 125.],\n",
       "           [ 38., 121., 157.,  82., 137.]],\n",
       " \n",
       "          [[229.,  44.,  68., 251., 156.],\n",
       "           [  0.,  82., 116., 143.,   5.],\n",
       "           [ 40.,  39., 203., 165., 152.],\n",
       "           [ 42.,  55.,  95., 156., 201.],\n",
       "           [111.,  20., 223., 183., 254.]]],\n",
       " \n",
       " \n",
       "         [[[  6.,  86.,  16.,  95., 113.],\n",
       "           [209., 196., 233., 252., 114.],\n",
       "           [ 96.,  14., 173.,  44., 198.],\n",
       "           [ 88.,  21.,  12., 199., 203.],\n",
       "           [135.,  85.,  12.,  16., 187.]],\n",
       " \n",
       "          [[234., 218.,  50.,  49.,  83.],\n",
       "           [252., 180., 123.,  34.,  54.],\n",
       "           [ 88.,  97., 240.,  63.,  68.],\n",
       "           [ 72., 131., 213.,  32., 166.],\n",
       "           [ 69.,  67., 228.,  84.,  30.]],\n",
       " \n",
       "          [[130.,  50.,  32., 190., 188.],\n",
       "           [107., 116., 146., 236., 235.],\n",
       "           [ 71., 221., 118., 138.,  38.],\n",
       "           [248.,  91.,  65., 107., 232.],\n",
       "           [201., 235.,  26.,  29.,  76.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[100., 117., 176., 172., 222.],\n",
       "           [122.,   5.,  63., 185., 187.],\n",
       "           [148., 195., 178.,  97.,  62.],\n",
       "           [ 99.,  59., 209.,  30., 116.],\n",
       "           [115.,  42., 126.,  28., 228.]],\n",
       " \n",
       "          [[239., 211., 139., 230.,  92.],\n",
       "           [109., 171., 185., 252., 124.],\n",
       "           [186.,  51.,  74.,  22., 137.],\n",
       "           [146., 206., 182., 226., 113.],\n",
       "           [252.,  55., 170., 224., 217.]],\n",
       " \n",
       "          [[186.,  52.,   6.,  83., 242.],\n",
       "           [ 78.,  80.,   2., 230.,  33.],\n",
       "           [ 71.,  47., 182., 236., 129.],\n",
       "           [ 16., 179., 192., 217.,  63.],\n",
       "           [144., 119., 152., 131., 132.]]],\n",
       " \n",
       " \n",
       "         [[[ 82., 109.,  10., 144., 151.],\n",
       "           [ 24.,  21.,  22., 135., 200.],\n",
       "           [ 49.,  88.,  74.,  84.,  19.],\n",
       "           [ 57., 216., 177., 219., 215.],\n",
       "           [231., 177., 237.,  36., 177.]],\n",
       " \n",
       "          [[248.,  37., 218., 132.,  26.],\n",
       "           [197.,   3., 220., 128.,  10.],\n",
       "           [162., 161., 252.,   2.,  33.],\n",
       "           [180.,  60., 140.,  50., 115.],\n",
       "           [111., 171., 127.,  35., 153.]],\n",
       " \n",
       "          [[104., 187.,  47.,   8., 125.],\n",
       "           [132., 117.,  42., 184., 226.],\n",
       "           [209., 106., 170.,  73., 173.],\n",
       "           [202.,  56.,  80., 254., 156.],\n",
       "           [209., 235.,  72., 119.,  43.]]],\n",
       " \n",
       " \n",
       "         [[[ 97., 115., 243.,  85.,  92.],\n",
       "           [ 69., 177.,  80.,  57., 161.],\n",
       "           [ 56., 126., 124., 205., 103.],\n",
       "           [170., 205., 186., 239.,  34.],\n",
       "           [173., 215.,  85., 226., 159.]],\n",
       " \n",
       "          [[ 69.,  21., 130., 185., 203.],\n",
       "           [ 69.,  99.,  59., 222.,  76.],\n",
       "           [  7., 120.,  62., 152., 181.],\n",
       "           [  4., 226.,  72.,  49.,  88.],\n",
       "           [ 39., 135.,  14., 176., 127.]],\n",
       " \n",
       "          [[153., 176., 168., 228., 120.],\n",
       "           [191., 191., 131., 116., 101.],\n",
       "           [103., 213., 111.,  54., 190.],\n",
       "           [125., 154., 123., 111.,  36.],\n",
       "           [  6., 234.,  46., 203., 165.]]]]),\n",
       " tensor([153., 184., 165., 254., 101.,  77., 127., 138., 215., 118., 184.,  53.,\n",
       "          60.,  61.,   0.,  52.,  94., 220.,  46., 230.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.quantize_layer(net.conv1, mode='asymmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = q.quantize_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12.,   6.,  89.,  ..., 155., 200.,  22.],\n",
       "        [160., 135., 148.,  ...,  51., 198., 244.],\n",
       "        [207.,  65., 132.,  ..., 170., 146., 241.],\n",
       "        ...,\n",
       "        [181., 106., 227.,  ..., 127., 217.,   0.],\n",
       "        [179.,  90., 164.,  ..., 186.,  57.,  26.],\n",
       "        [109., 210., 151.,  ..., 197., 184., 231.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, mnist=True):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_folding(conv, bn):\n",
    "    w = conv.weight\n",
    "    b = conv.bias\n",
    "    \n",
    "    gamma = bn.weight\n",
    "    beta = bn.bias\n",
    "    mean = bn.running_mean\n",
    "    var = torch.sqrt(bn.running_var + bn.eps)\n",
    "    \n",
    "    w_f = w * (gamma/var).view(-1,1,1,1)\n",
    "    b_f = gamma * ((b - mean)/var) + beta\n",
    "    return w_f, b_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_f, b_f = bn_folding(net.conv1, net.bn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_f(nn.Module):\n",
    "    def __init__(self, mnist=True):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f = Net_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_f(\n",
       "  (conv1): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_f.conv1.weight.data = w_f\n",
    "net_f.conv1.bias.data = b_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.lt(torch.abs(torch.add(net(x), -net_f(x))), 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse(conv, bn):\n",
    "    w = conv.weight\n",
    "    b = conv.bias\n",
    "    \n",
    "    gamma = bn.weight\n",
    "    beta = bn.bias\n",
    "    mean = bn.running_mean\n",
    "    var = torch.sqrt(bn.running_var + bn.eps)\n",
    "    \n",
    "    w_f = w * (gamma/var).view(-1,1,1,1)\n",
    "    b_f = gamma * ((b - mean)/var) + beta\n",
    "    \n",
    "    fused_conv = nn.Conv2d(conv.in_channels,\n",
    "                         conv.out_channels,\n",
    "                         conv.kernel_size,\n",
    "                         conv.stride,\n",
    "                         conv.padding,\n",
    "                         bias=True)\n",
    "    fused_conv.weight = nn.Parameter(w)\n",
    "    fused_conv.bias = nn.Parameter(b)\n",
    "    return fused_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "        self.fuse = fuse(self.conv1, self.bn1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x, fusion=False):\n",
    "        if fusion:\n",
    "            x = self.fuse(x)           \n",
    "        else: \n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "        x = self.relu1(x)    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "net(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "net(x, True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
